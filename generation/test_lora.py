#!/usr/bin/env python3
"""
LoRAæ¨¡å‹æ¸¬è©¦è…³æœ¬
ç”¨æ–¼æ¸¬è©¦è¨“ç·´å¥½çš„LoRA adapterçš„TTSæ•ˆæœ
"""

import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import outetts

def test_lora_model():
    """æ¸¬è©¦LoRAæ¨¡å‹çš„èªéŸ³ç”Ÿæˆæ•ˆæœ"""
    
    # è¨­å®šè·¯å¾‘
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    model_dir = os.path.join(base_dir, "models", "Llama-OuteTTS-1.0-1B")
    lora_path = os.path.join(base_dir, "outputs", "lora_model")
    
    print(f"ğŸ” æª¢æŸ¥LoRAæ¨¡å‹è·¯å¾‘: {lora_path}")
    
    # æª¢æŸ¥LoRAæ¨¡å‹æ˜¯å¦å­˜åœ¨
    if not os.path.exists(lora_path):
        print(f"âŒ LoRAæ¨¡å‹ä¸å­˜åœ¨æ–¼: {lora_path}")
        return False
    
    # åˆ—å‡ºLoRAæ¨¡å‹æª”æ¡ˆ
    lora_files = [f for f in os.listdir(lora_path) if f.endswith(('.bin', '.safetensors', '.json'))]
    print(f"ğŸ“ æ‰¾åˆ°LoRAæª”æ¡ˆ: {lora_files}")
    
    if not lora_files:
        print("âŒ LoRAè³‡æ–™å¤¾æ˜¯ç©ºçš„ï¼Œæ²’æœ‰æ‰¾åˆ°æ¨¡å‹æª”æ¡ˆ")
        return False
    
    try:
        print("ğŸš€ è¼‰å…¥åŸºç¤æ¨¡å‹...")
        # è¼‰å…¥åŸºç¤æ¨¡å‹å’Œtokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            model_dir, 
            trust_remote_code=True,
            local_files_only=True
        )
        
        base_model = AutoModelForCausalLM.from_pretrained(
            model_dir,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True,
            local_files_only=True
        )
        
        print("ğŸ”§ è¼‰å…¥LoRA adapter...")
        # è¼‰å…¥LoRA adapter
        model = PeftModel.from_pretrained(base_model, lora_path)
        
        print("âœ… LoRAæ¨¡å‹è¼‰å…¥æˆåŠŸï¼")
        print(f"ğŸ“Š LoRAé…ç½®: r={model.peft_config['default'].r}, alpha={model.peft_config['default'].lora_alpha}")
        
        # åˆä½µadapteråˆ°åŸºç¤æ¨¡å‹ (å¯é¸)
        print("ğŸ”€ åˆä½µLoRA weights...")
        model = model.merge_and_unload()
        
        print("ğŸ’¾ æš«æ™‚ä¿å­˜åˆä½µå¾Œçš„æ¨¡å‹...")
        temp_model_path = os.path.join(base_dir, "outputs", "temp_merged_model")
        os.makedirs(temp_model_path, exist_ok=True)
        model.save_pretrained(temp_model_path)
        tokenizer.save_pretrained(temp_model_path)
        
        return temp_model_path
        
    except Exception as e:
        print(f"âŒ è¼‰å…¥LoRAæ¨¡å‹å¤±æ•—: {e}")
        return False

def test_generation(model_path):
    """ä½¿ç”¨åˆä½µå¾Œçš„æ¨¡å‹é€²è¡ŒèªéŸ³ç”Ÿæˆæ¸¬è©¦"""
    
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    
    try:
        print("ğŸ¤ åˆå§‹åŒ–OuteTTS interface...")
        
        # ä½¿ç”¨åˆä½µå¾Œçš„æ¨¡å‹
        interface = outetts.Interface(
            config=outetts.ModelConfig(
                model_path=model_path,
                tokenizer_path=model_path,
                interface_version=outetts.InterfaceVersion.V3,
                backend=outetts.Backend.HF,
            )
        )
        
        print("ğŸ—£ï¸ è¼‰å…¥èªªè©±è€…...")
        # å˜—è©¦è¼‰å…¥è‡ªå®šç¾©èªªè©±è€…ï¼Œå¦‚æœæ²’æœ‰å‰‡ä½¿ç”¨é è¨­
        speaker_path = os.path.join(base_dir, "speaker", "uep_speaker.json")
        if os.path.exists(speaker_path):
            speaker = interface.load_speaker(speaker_path)
            print("âœ… è¼‰å…¥è‡ªå®šç¾©èªªè©±è€…")
        else:
            speaker = interface.load_default_speaker("EN-FEMALE-1-NEUTRAL")
            print("âœ… ä½¿ç”¨é è¨­èªªè©±è€…")
        
        # æ¸¬è©¦ç”¨çš„æ–‡æœ¬
        test_texts = [
            "Hello! This is a test of our fine-tuned TTS model.",
            "The weather is beautiful today, isn't it?",
            "I hope this training has improved the voice quality.",
            "é€™æ˜¯ä¸€å€‹æ¸¬è©¦èªå¥ï¼Œç”¨ä¾†æª¢é©—ä¸­è‹±æ–‡æ··åˆçš„æ•ˆæœã€‚",
            "Wow! I see. Okay! Hmm. Alright!"  # ä½¿ç”¨è¨“ç·´æ•¸æ“šä¸­çš„çŸ­èª
        ]
        
        print("ğŸµ é–‹å§‹ç”Ÿæˆæ¸¬è©¦èªéŸ³...")
        os.makedirs(os.path.join(base_dir, "outputs", "samples"), exist_ok=True)
        
        for i, text in enumerate(test_texts):
            print(f"  ğŸ”Š ç”Ÿæˆç¬¬ {i+1}/{len(test_texts)} å€‹æ¨£æœ¬: {text[:30]}...")
            
            try:
                output = interface.generate(
                    outetts.GenerationConfig(
                        text=text,
                        speaker=speaker,
                        temperature=0.4,
                        repetition_penalty=1.1,
                        top_k=40,
                        top_p=0.9
                    )
                )
                
                output_file = os.path.join(base_dir, "outputs", "samples", f"lora_test_{i+1}.wav")
                output.save(output_file)
                print(f"    âœ… å·²ä¿å­˜: {output_file}")
                
            except Exception as e:
                print(f"    âŒ ç”Ÿæˆå¤±æ•—: {e}")
        
        print("ğŸ‰ æ¸¬è©¦å®Œæˆï¼è«‹æª¢æŸ¥ outputs/samples/ è³‡æ–™å¤¾ä¸­çš„éŸ³é »æª”æ¡ˆ")
        return True
        
    except Exception as e:
        print(f"âŒ èªéŸ³ç”Ÿæˆæ¸¬è©¦å¤±æ•—: {e}")
        return False

def compare_with_base_model():
    """èˆ‡åŸºç¤æ¨¡å‹é€²è¡Œå°æ¯”æ¸¬è©¦"""
    
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    model_dir = os.path.join(base_dir, "models", "Llama-OuteTTS-1.0-1B")
    
    try:
        print("ğŸ”„ è¼‰å…¥åŸå§‹åŸºç¤æ¨¡å‹é€²è¡Œå°æ¯”...")
        
        interface = outetts.Interface(
            config=outetts.ModelConfig(
                model_path=model_dir,
                tokenizer_path=model_dir,
                interface_version=outetts.InterfaceVersion.V3,
                backend=outetts.Backend.HF,
            )
        )
        
        speaker = interface.load_default_speaker("EN-FEMALE-1-NEUTRAL")
        
        # ä½¿ç”¨åŒä¸€å€‹æ¸¬è©¦æ–‡æœ¬
        test_text = "Hello! This is a comparison test between the base model and our fine-tuned model."
        
        output = interface.generate(
            outetts.GenerationConfig(
                text=test_text,
                speaker=speaker,
                temperature=0.4,
                repetition_penalty=1.1,
                top_k=40,
                top_p=0.9
            )
        )
        
        output_file = os.path.join(base_dir, "outputs", "samples", "base_model_comparison.wav")
        output.save(output_file)
        print(f"âœ… åŸºç¤æ¨¡å‹å°æ¯”æ¨£æœ¬å·²ä¿å­˜: {output_file}")
        
    except Exception as e:
        print(f"âŒ åŸºç¤æ¨¡å‹æ¸¬è©¦å¤±æ•—: {e}")

def cleanup_temp_files():
    """æ¸…ç†è‡¨æ™‚æª”æ¡ˆ"""
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    temp_path = os.path.join(base_dir, "outputs", "temp_merged_model")
    
    if os.path.exists(temp_path):
        import shutil
        shutil.rmtree(temp_path)
        print("ğŸ§¹ å·²æ¸…ç†è‡¨æ™‚æª”æ¡ˆ")

def main():
    """ä¸»å‡½æ•¸"""
    print("=" * 60)
    print("ğŸ§ª OuteTTS LoRA æ¨¡å‹æ¸¬è©¦å·¥å…·")
    print("=" * 60)
    
    # 1. æ¸¬è©¦LoRAæ¨¡å‹è¼‰å…¥
    merged_model_path = test_lora_model()
    if not merged_model_path:
        print("âŒ ç„¡æ³•è¼‰å…¥LoRAæ¨¡å‹ï¼Œè«‹æª¢æŸ¥æ¨¡å‹æª”æ¡ˆ")
        return
    
    # 2. é€²è¡ŒèªéŸ³ç”Ÿæˆæ¸¬è©¦
    print("\n" + "-" * 40)
    success = test_generation(merged_model_path)
    
    # 3. èˆ‡åŸºç¤æ¨¡å‹å°æ¯” (å¯é¸)
    print("\n" + "-" * 40)
    compare_with_base_model()
    
    # 4. æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
    cleanup_temp_files()
    
    if success:
        print("\nğŸ‰ æ¸¬è©¦å®Œæˆï¼")
        print("ğŸ“ è«‹æª¢æŸ¥ outputs/samples/ è³‡æ–™å¤¾ä¸­çš„ç”ŸæˆéŸ³é »")
        print("ğŸ’¡ å»ºè­°ï¼š")
        print("   - æ¯”è¼ƒ lora_test_*.wav èˆ‡ base_model_comparison.wav")
        print("   - è©•ä¼°éŸ³è³ªã€è‡ªç„¶åº¦å’Œæº–ç¢ºæ€§")
        print("   - å¦‚æœæ•ˆæœæ»¿æ„ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨")
        print("   - å¦‚æœéœ€è¦æ”¹é€²ï¼Œå¯ä»¥è€ƒæ…®çºŒæ¥è¨“ç·´")
    else:
        print("\nâŒ æ¸¬è©¦éç¨‹ä¸­å‡ºç¾å•é¡Œï¼Œè«‹æª¢æŸ¥éŒ¯èª¤ä¿¡æ¯")

if __name__ == "__main__":
    main()
