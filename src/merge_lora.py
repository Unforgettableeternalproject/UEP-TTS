"""
åˆä½µLoRA adapteråˆ°base modelçš„è…³æœ¬
å°‡è¨“ç·´å¥½çš„LoRAæ¬Šé‡åˆä½µåˆ°åŸå§‹æ¨¡å‹ä¸­ï¼Œå‰µå»ºä¸€å€‹æ–°çš„å®Œæ•´æ¨¡å‹
"""
import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import shutil

def merge_lora_to_base():
    """å°‡LoRA adapteråˆä½µåˆ°base model"""
    
    # è·¯å¾‘è¨­å®š
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    base_model_path = os.path.join(base_dir, "models", "Llama-OuteTTS-1.0-1B")
    lora_path = os.path.join(base_dir, "outputs", "lora_model")
    merged_model_path = os.path.join(base_dir, "models", "Llama-OuteTTS-1.0-1B-Finetuned")
    
    print(f"Base model: {base_model_path}")
    print(f"LoRA adapter: {lora_path}")
    print(f"Output path: {merged_model_path}")
    
    # æª¢æŸ¥è·¯å¾‘
    if not os.path.exists(lora_path):
        print("âŒ LoRAæ¨¡å‹ä¸å­˜åœ¨")
        return False
    
    try:
        print("ğŸ”„ è¼‰å…¥base model...")
        # è¼‰å…¥base model
        tokenizer = AutoTokenizer.from_pretrained(
            base_model_path, 
            trust_remote_code=True,
            local_files_only=True
        )
        
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True,
            local_files_only=True
        )
        
        print("ğŸ”„ è¼‰å…¥LoRA adapter...")
        # è¼‰å…¥LoRAæ¨¡å‹
        model = PeftModel.from_pretrained(base_model, lora_path)
        
        print("ğŸ”„ åˆä½µLoRAæ¬Šé‡...")
        # åˆä½µLoRAæ¬Šé‡åˆ°base model
        merged_model = model.merge_and_unload()
        
        print("ğŸ’¾ ä¿å­˜åˆä½µå¾Œçš„æ¨¡å‹...")
        # å‰µå»ºè¼¸å‡ºç›®éŒ„
        os.makedirs(merged_model_path, exist_ok=True)
        
        # ä¿å­˜åˆä½µå¾Œçš„æ¨¡å‹
        merged_model.save_pretrained(merged_model_path)
        tokenizer.save_pretrained(merged_model_path)
        
        # è¤‡è£½é…ç½®æ–‡ä»¶
        config_files = ['config.json', 'generation_config.json', 'special_tokens_map.json']
        for config_file in config_files:
            src = os.path.join(base_model_path, config_file)
            dst = os.path.join(merged_model_path, config_file)
            if os.path.exists(src):
                shutil.copy2(src, dst)
                print(f"  âœ… è¤‡è£½: {config_file}")
        
        print(f"ğŸ‰ åˆä½µå®Œæˆï¼æ–°æ¨¡å‹ä¿å­˜åœ¨: {merged_model_path}")
        
        # æª¢æŸ¥ä¿å­˜çš„æ–‡ä»¶
        saved_files = os.listdir(merged_model_path)
        print(f"ğŸ“ ä¿å­˜çš„æ–‡ä»¶: {saved_files}")
        
        return True
        
    except Exception as e:
        print(f"âŒ åˆä½µéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        return False

if __name__ == "__main__":
    print("=" * 50)
    print("LoRAåˆ°Base Modelåˆä½µå·¥å…·")
    print("=" * 50)
    
    success = merge_lora_to_base()
    
    if success:
        print("\nâœ¨ æˆåŠŸï¼ç¾åœ¨ä½ å¯ä»¥:")
        print("1. åœ¨OuteTTSä¸­ä½¿ç”¨åˆä½µå¾Œçš„æ¨¡å‹")
        print("2. ä¿®æ”¹generationè…³æœ¬ä½¿ç”¨æ–°çš„æ¨¡å‹è·¯å¾‘")
        print("3. æ¯”è¼ƒåŸå§‹æ¨¡å‹å’Œå¾®èª¿æ¨¡å‹çš„æ•ˆæœ")
    else:
        print("\nğŸ’¡ å¦‚æœåˆä½µå¤±æ•—ï¼Œå¯ä»¥è€ƒæ…®:")
        print("1. æª¢æŸ¥LoRAæ¨¡å‹æ˜¯å¦æ­£ç¢ºä¿å­˜")
        print("2. ç¢ºèªPEFTç‰ˆæœ¬ç›¸å®¹æ€§")
        print("3. ç›´æ¥ä½¿ç”¨ç•¶å‰çš„base model")
